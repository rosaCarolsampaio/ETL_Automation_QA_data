{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07838a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T19:28:46.668596Z",
     "iopub.status.busy": "2022-09-25T19:28:46.666345Z",
     "iopub.status.idle": "2022-09-25T19:28:46.683558Z",
     "shell.execute_reply": "2022-09-25T19:28:46.677602Z"
    },
    "papermill": {
     "duration": 0.033146,
     "end_time": "2022-09-25T19:28:46.689093",
     "exception": false,
     "start_time": "2022-09-25T19:28:46.655947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import nbformat as nbf\n",
    "# from glob import glob\n",
    "\n",
    "# notebooks = glob(\"./**/*.ipynb\", recursive=True)\n",
    "\n",
    "# text_search_dict = {\n",
    "#     \"parameters\": [{\"db2\": \"db-dev\"}],\n",
    "# }\n",
    "# pprint(len(notebooks))\n",
    "\n",
    "# for tag in notebooks:\n",
    "#     ntbk = nbf.read(tag, nbf.NO_CONVERT)\n",
    "\n",
    "#     for cell in ntbk.cells:\n",
    "#         cell_tags = cell.get('metadata', {}).get('tags', [])\n",
    "#         for key, val in text_search_dict.items():\n",
    "#             if key in cell['source']:\n",
    "#                 if val not in cell_tags:\n",
    "#                     cell_tags.append(val)\n",
    "#         if len(cell_tags)> 0:\n",
    "#             cell['metadata']['tags'] = cell_tags\n",
    "#     nbf.write(ntbk, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8381b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T19:28:46.702623Z",
     "iopub.status.busy": "2022-09-25T19:28:46.702214Z",
     "iopub.status.idle": "2022-09-25T19:28:46.707687Z",
     "shell.execute_reply": "2022-09-25T19:28:46.706682Z"
    },
    "papermill": {
     "duration": 0.012374,
     "end_time": "2022-09-25T19:28:46.709730",
     "exception": false,
     "start_time": "2022-09-25T19:28:46.697356",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# # db = \"db-dev\"\n",
    "\n",
    "# def connection(spark: SparkSession) -> None:\n",
    "#     conn = psycopg2.connect(database={db}, user=\"postgres\", password=\"rosa\", host=\"localhost\")\n",
    "#     cur = conn.cursor()\n",
    "#     cur.execute('SELECT * FROM \"Items\"')\n",
    "    \n",
    "#     records = cur.fetchall()\n",
    "#     print(records)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .appName(\"Python Spark SQL basic example\") \\\n",
    "#         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "#     connection(spark)\n",
    "\n",
    "#     spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250f829a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T19:28:46.713989Z",
     "iopub.status.busy": "2022-09-25T19:28:46.713742Z",
     "iopub.status.idle": "2022-09-25T19:28:46.719851Z",
     "shell.execute_reply": "2022-09-25T19:28:46.719332Z"
    },
    "papermill": {
     "duration": 0.009856,
     "end_time": "2022-09-25T19:28:46.721252",
     "exception": false,
     "start_time": "2022-09-25T19:28:46.711396",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "db = \"db-dev\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ba6586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-25T19:28:46.725007Z",
     "iopub.status.busy": "2022-09-25T19:28:46.724786Z",
     "iopub.status.idle": "2022-09-25T19:28:55.208397Z",
     "shell.execute_reply": "2022-09-25T19:28:55.206137Z"
    },
    "papermill": {
     "duration": 8.489829,
     "end_time": "2022-09-25T19:28:55.212525",
     "exception": false,
     "start_time": "2022-09-25T19:28:46.722696",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/25 20:28:48 WARN Utils: Your hostname, gitlab resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp0s20f3)\n",
      "22/09/25 20:28:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/rosasilva/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rosasilva/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fa9c5e6f-71b5-4fcb-b8a0-f4a6759c65cd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.3.6 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 152ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.3.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fa9c5e6f-71b5-4fcb-b8a0-f4a6759c65cd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/25 20:28:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n",
      "| id|                name|is_healthy|\n",
      "+---+--------------------+----------+\n",
      "|  2|iPhone           ...|      null|\n",
      "|  3|coffee           ...|      null|\n",
      "|  5|Ã¡gua             ...|      null|\n",
      "|  1|playstation      ...|      true|\n",
      "+---+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|is_healthy|\n",
      "+---+----+----------+\n",
      "|  0|   0|         3|\n",
      "+---+----+----------+\n",
      "\n",
      "22/09/25 20:28:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, is_healthy: boolean, name: string, count: bigint]\n",
      "\n",
      "\n",
      "\n",
      " All field haven't null value \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext# contents of `papermill_runner.py`\n",
    "import papermill as pm\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark_test import assert_pyspark_df_equal\n",
    "import pytest\n",
    "import warnings\n",
    "\n",
    "findspark.add_packages('org.postgresql:postgresql:42.3.6')\n",
    "db = \"db-dev\"\n",
    "url = \"jdbc:postgresql://localhost/\"+ db\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.3.6.jar\").master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()\n",
    "df = spark.read.format(\"jdbc\").option(\"url\",url ).option(\"dbtable\", 'items').option(\"user\", \"postgres\").option(\"password\", \"rosa\").option(\"driver\", \"org.postgresql.Driver\").load()\n",
    "df.show()\n",
    "\n",
    "df2 = spark.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", 'items').option(\"user\", \"postgres\").option(\"password\", \"rosa\").option(\"driver\", \"org.postgresql.Driver\").load()\n",
    "\n",
    "haveAnyNullValue = df.select([count(when(col(c) == \" \", c).when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "haveAnyNullValue.show()\n",
    "array = haveAnyNullValue.collect()\n",
    "\n",
    "def test_validate_and_capitalize():\n",
    "    section_folders = verify()       \n",
    "    section_name <= 1                                  \n",
    "    with pytest.raises(Exception) as e:\n",
    "        verify(section_name, section_folders) \n",
    "    assert \"have column with null value in str(e.value)\"\n",
    "    assert e.type == AssertionError                                 \n",
    "\n",
    "def verify(spark: SparkSession) -> None: \n",
    "    print(df.groupBy(sorted(df.columns)).count())\n",
    "    print(\"\\n\")\n",
    "    assert 'rosa' == 'rosa'\n",
    "    assert df.groupBy(sorted(df.columns)).count()  != 0\n",
    "    \n",
    "    totalCollumns = len(haveAnyNullValue.columns)\n",
    "    for i in range(totalCollumns) :\n",
    "        assert array[0][i] == 0  or array[0][i] == 3\n",
    "    \n",
    "    # assert array[0][1] == 0\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession .builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "    verify(spark)\n",
    "    print(\"\\n All field haven't null value \\n\")\n",
    "\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.116385,
   "end_time": "2022-09-25T19:28:57.841649",
   "environment_variables": {},
   "exception": null,
   "input_path": "./notebooks/connection.ipynb",
   "output_path": "./reports/connection_report.ipynb",
   "parameters": {
    "db": "db-dev"
   },
   "start_time": "2022-09-25T19:28:45.725264",
   "version": "2.4.0"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}